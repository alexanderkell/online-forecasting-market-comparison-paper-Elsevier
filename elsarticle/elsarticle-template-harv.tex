%% 
%% Copyright 2007-2019 Elsevier Ltd
%% 
%% This file is part of the 'Elsarticle Bundle'.
%% ---------------------------------------------
%% 
%% It may be distributed under the conditions of the LaTeX Project Public
%% License, either version 1.2 of this license or (at your option) any
%% later version.  The latest version of this license is in
%%    http://www.latex-project.org/lppl.txt
%% and version 1.2 or later is part of all distributions of LaTeX
%% version 1999/12/01 or later.
%% 
%% The list of all files belonging to the 'Elsarticle Bundle' is
%% given in the file `manifest.txt'.
%% 
%% Template article for Elsevier's document class `elsarticle'
%% with harvard style bibliographic references

%\documentclass[preprint,12pt,authoryear]{elsarticle}

%% Use the option review to obtain double line spacing
%% \documentclass[authoryear,preprint,review,12pt]{elsarticle}

%% Use the options 1p,twocolumn; 3p; 3p,twocolumn; 5p; or 5p,twocolumn
%% for a journal layout:
%% \documentclass[final,1p,times,authoryear]{elsarticle}
%% \documentclass[final,1p,times,twocolumn,authoryear]{elsarticle}
%% \documentclass[final,3p,times,authoryear]{elsarticle}
\documentclass[final,3p,times,twocolumn,numbers]{elsarticle}
%% \documentclass[final,5p,times,authoryear]{elsarticle}
%% \documentclass[final,5p,times,twocolumn,authoryear]{elsarticle}

%% For including figures, graphicx.sty has been loaded in
%% elsarticle.cls. If you prefer to use the old commands
%% please give \usepackage{epsfig}

%% The amssymb package provides various useful mathematical symbols
\usepackage{amssymb}
%% The amsthm package provides extended theorem environments
\usepackage{amsthm}
\usepackage{amsmath}
\usepackage{mathtools}
 \usepackage{booktabs}
\usepackage{mhchem}
\usepackage{xcolor}
\usepackage{csvsimple,booktabs}
\usepackage{graphicx}

\usepackage{algorithm}
\usepackage{algorithmicx}
\usepackage{algpseudocode}

\usepackage{subcaption}
\usepackage{mwe}

%% The lineno packages adds line numbers. Start line numbering with
%% \begin{linenumbers}, end it with \end{linenumbers}. Or switch it on
%% for the whole article with \linenumbers.
%% \usepackage{lineno}

\journal{Sustainable Computing: Informatics and Systems}

\begin{document}

\begin{frontmatter}

%% Title, authors and addresses

%% use the tnoteref command within \title for footnotes;
%% use the tnotetext command for theassociated footnote;
%% use the fnref command within \author or \address for footnotes;
%% use the fntext command for theassociated footnote;
%% use the corref command within \author for corresponding author footnotes;
%% use the cortext command for theassociated footnote;
%% use the ead command for the email address,
%% and the form \ead[url] for the home page:
 \title{The impact of online machine-learning methods on long-term investment decisions and generator utilization in electricity markets}
% \tnotetext[label1]{}
 \author{Alexander J. M. Kell}
 \ead{a.kell2@newcastle.ac.uk}
% \ead[url]{home page}
% \fntext[label2]{}
% \cortext[cor1]{}
% \address{Address\fnref{label3}}
% \fntext[label3]{}

%\title{Validating a long-term electricity market model}

%% use optional labels to link authors explicitly to addresses:
%% \author[label1,label2]{}
%% \address[label1]{}
%% \address[label2]{}

\author{A. Stephen McGough, Matthew Forshaw}

\address{School of Computing, Newcastle University, Newcastle-upon-Tyne, United Kingdom}

\begin{abstract}
%% Text of abstract

\end{abstract}
%
%%%Graphical abstract
%\begin{graphicalabstract}
%\includegraphics{grabs}
%Hello test
%\end{graphicalabstract}
%
%%%Research highlights
%\begin{highlights}
%\item Validating a model
%\item Optimisation
%\item Scenario modelling
%\end{highlights}

\begin{keyword}
%% keywords here, in the form: keyword \sep keyword
Long-Term Energy Modelling \sep Online learning \sep Machine learning \sep Market investment \sep Climate Change
%% PACS codes here, in the form: \PACS code \sep code

%% MSC codes here, in the form: \MSC code \sep code
%% or \MSC[2008] code \sep code (2000 is the default)

\end{keyword}

\end{frontmatter}

%% \linenumbers

%% main text
\section{Introduction}
\label{sec:intro}

The integration of higher proportions of intermittent renewable energy sources (IRES) in the smart grid will mean that the forecasting of electricity demand will become increasingly important. This is due to the fact that supply must mean demand at all times and that IRES are less predictable than dispatchable energy sources such as coal and combined-cycle gas turbines (CCGTs) \cite{Lu1993}.

Typically, peaker plants, such as reciprocal gas engines, are used to fill fluctuations in demand, that had not been previously planned for. These peaker plants are typically expensive to run and have higher greenhouse gas emissions than their non-peaker counterparts \cite{Mahmood2014}. 

To reduce reliance on peaker plants, it is helpful to know how much electricity demand there will be in the future so that more efficient plants can be used to meet this demand. To aid in this, machine learning and statistical techniques have been used to accurately predict demand based on several different factors and data sources, such as weather, day of the week and holidays \cite{Kell2018a, Al-Musaylh2018, Vrablecova2017, Hong2014}. 

%- Why the current approaches / systems don't solve the problem


While various studies have looked at how to predict electricity demand at various time horizons with the highest accuracy \cite{Singh2012}, the impact that the differing methods used have on a long-term electricity market has been studied to a lesser degree. In this paper, we compare several machine learning and statistical techniques to forecast 24 hours ahead to simulate a day-ahead market. 

In addition to this, we use the long-term agent-based model, ElecSim \cite{Kell, Kell2020}, to simulate the impact of different forecasting methods on long-term investments, power plant usage and carbon emissions from the year 2018 to 2035 in the United Kingdom. 


%- What is the innovation in this work

To compare the impact of different methods, we trial both online and offline machine learning and statistical techniques. Online learning methods can learn from novel data while maintaining what was learnt from previous data. This type of statistical modelling is useful for non-stationary datasets, and time-series data where recalculation of a model would take a prohibitive amount of time. We trial different algorithms and split times of the year into seasons, weekdays, weekends and holidays.

%- A short description of the solution


Using these methods, we take the error distributions, or residuals, and fit a variety of distributions. The distribution that best fits the respective residuals is then used and sampled from to adjust the demand in the model ElecSim. We then observe the differences in carbon emissions, and power plants invested in and utilised with each of the different statistical and machine learning methods to a base case.




%- What are the key take-home messages

We show that online learning has a significant impact on reducing the error for predicting electricity consumption a day ahead when compared to traditional offline learning techniques.

We show that the forecasting algorithm has a non-negligible effect on carbon emissions and use of coal, onshore, photovoltaics, reciprocal gas engines and CCGT. Specifically, the amount of coal, photovoltaics, and reciprocal gas used from 2018 to 2035 was proportional to the median absolute error, while both onshore and offshore wind are inversely proportional to the median absolute error.

Total investments in coal, offshore and photovoltaics are proportional to the median absolute error, while investments in CCGT, onshore and reciprocal gas engines inversely proportional.



%- Outline of the rest of the work

In Section \ref{sec:lit-review} we review the literature. We introduce the model ElecSim, and the methods used in Section \ref{sec:material}. The Methods section is shown in Section \ref{sec:methods}, results in Section \ref{sec:results}. We discuss and conclude our work in Sections \ref{sec:discussion} and \ref{sec:conclusion} respectively.

\section{Literature Review}
\label{sec:lit-review}

Multiple papers have looked at demand-side forecasting \cite{Singh2012}. These include both artificial intelligence \cite{Kim2000, Tiong2008,Quilumba2014} and statistical techniques \cite{Nazarko2005ARIMAApproach,Huang2003,Nguyen2017}. However, the impact of online learning has been discussed with less frequency. In addition to this, our research shows the impact of different algorithms on investments made, electricity sources contribution and carbon emissions over a 17 year period using the long-term electricity market agent-based model, ElecSim.

The have been several studies in diverse applications on the use of online machine learning to predict time-series data. Johansson \textit{et al}. apply online machine learning algorithms for heat demand forecasting \cite{Johansson2017}. They find that their demand predictions display robust behaviour within acceptable error margins. They find that artificial neural networks (ANNs) provide the best forecasting ability of the standard algorithms and can handle data outside of the training set.

Baram \textit{et al.} combine an ensemble of active learners by developing an active-learning master algorithm \cite{Baram2003}. To achieve this, they propose a simple maximum entropy criterion that provides effective estimates in realistic settings. Their active-learning master algorithm is empirically shown to, in some cases, outperform the best algorithm in the ensemble on a range of classification problems.

An extension of the FLORA algorithm is proposed by Schmitt \textit{et al} in \cite{Schmitt2008}. Their FLORA-MC enhances the FLORA algorithm for multi-classification and numerical input values. They use this algorithm for an ambient computing application. Ambient computing is where computing and communication merges into every day life. They find that their model outperforms traditional offline learners by orders of magnitude.

Pindoriya \textit{et al}. trial a number of different machine learning methods such an adaptive wavelet neural network (AWNN). They find that AWNN has good prediction properties when compared to other forecasting techniques such as wavelet-ARIMA, multi-layer perceptron (MLP) and radial basis function (RBF) neural networks as well as the fuzzy neural network (FNN).


Goncalves Da Silva \textit{et al}. show the effect of prediction accuracy on local electricity markets \cite{GoncalvesDaSilva2014}. To this end, they compare forecasting of groups of consumers in comparison to single individuals. They trial the use of the Seasonal-Naïve and Holt-Winters algorithms and look at the effect that he errors have on trading in an intra-day electricity market of consumers and prosumers. They found that with a photovoltaic penetration of 50\%, over 10\% of the total generation capacity was uncapitalised and roughly 10, 25 and 28\% of the total traded volume were unnecessary buys, demand imbalances and unnecessary sells respectively. This represents energy that the participant has no control.



\section{Material}
\label{sec:material}

\subsection{Machine Learning}

Machine learning is a methodology for finding and describing structural patterns in data \cite{Witten2011}. Typically machine learning algorithms are algorithms which are trained using data. These generate models which can make predictions or decisions with regards to classification of data \cite{Johansson2017}. Machine learning algorithms can be used for both static datasets as well as streaming data.




\subsection{Online learning}

Online machine learning is an extension of machine learning algorithms, that can be used on dynamic datasets, such as time-series data. In traditional machine learning algorithms, when new data is obtained, the historical data must be used to retrain the entire model with new model. This can be costly both in terms of time and in computation power \cite{Li2016}. Online algorithms therefore  avoids the repeated retraining of data and improves the learning efficiency \cite{Rong2009}.

Examples of such algorithms are Passive Aggressive (PA) Regressor \cite{Gzik2014}, Linear Regression, Box-Cox Regressor \cite{Box1964}, K-Neighbors Regressor \cite{forgy65}, Softmax Regression, Multilayer perceptron regressor \cite{Hinton1989}. For our work, we trial the stated algorithms, in addition to a host of offline learning techniques. The online techniques trialled were Lasso regression \cite{Tibshirani1996a}, ridge regression \cite{GeladiPaul1994Mrac},  Elastic Net \cite{Geostatistics2010}, least angle regression \cite{Fike1988}, Extra Trees Regressor \cite{Fike1988}, Random Forest Regressor \cite{Breiman2001}, AdaBoost Regressor \cite{Freund1997}, Gradient Boosting Regressor \cite{316}, Support vector regression \cite{Cortes1995}, and offline versions of Multilayer perceptron regressor, K-Neighbors regressor, linear regression.

We trial the previously mentioned statistical and machine learning algorithms, and vary the parameters using grid search and cross-validation in the python package, scikit learn \cite{scikit-learn}.

\subsection{Algorithms}

In this section we give a brief outline of the most successful online learning algorithms in this work.

\subsubsection{Multilayer perceptron}


\begin{figure}
	\includegraphics[width=0.4\textwidth]{figures/methods/Kell_eEnergy_Fig1.eps}
	\caption{A three layer feed forward neural network.}
	\label{fig:mlp}
\end{figure}


Artificial Neural Networks are a model which can model non-linear relationships between input and output data \cite{Akaike1974}. A popular neural network is a feed forward multilayer perceptron. Fig. \ref{fig:mlp} shows a three layer feed forward neural network with a single output unit, \textit{k} hidden units, $n$ input units. $w_{ij}$ is the connection weight from the $i$th input unit to the $j$th hidden unit,  and $T_j$ is the connecting weight from the $j$th hidden unit to the output unit \cite{Pao2007}. These weights transform the input variables in the first layer to the output variable in the final layer using the training data. 

Typically, a dataset is split into three sections, the test set, training set and validation set. The training set is used to find the connection weights of the network, whilst the test set is used to determine the accuracy of the models. The validation set allows for an unbiased evaluation of the model whilst tuning the hyperparameters, and can avoid overfitting by stopping training if the error begins to increase.

For a univariate time series forecasting problem, suppose we have N observations $y_1, y_2, \ldots, y_N$ in the training set, 
\begin{equation}
y_{N+1}, y_{N+2}, \ldots, y_{N+m}
\end{equation}
\noindent in the test set and we are required to predict \textit{m} periods ahead \cite{Pao2007}. 

The training patterns are as follows:
\begin{align}
y_{p+m} & =f(y_p, y_{p-1},\ldots,y_1)\\
y_{p+m+1} & =f(y_{p+1}, y_{p},\ldots,y_2)\\
&\vdotswithin  \notag \\
y_{N} & =f(y_{N-m},y_{N-m-1},\ldots,y_{N-m-p+1})
\end{align}

\noindent where $f$ is the function made up of weights and activation functions in the trained neural network.

The $m$ testing patterns are 

\begin{align}
y_{N+1} & =f(y_{N+1-m}, y_{N-m},\ldots,y_{N-m-p+2})\\
y_{N+2} & =f(y_{N+2-m}, y_{N-m+1},\ldots,y_{N-m-p+3})\\
&\vdotswithin  \notag \\
y_{N+m} & =f(y_{N},y_{N-1},\ldots,y_{N-p+1})
\end{align}

The training objective is to minimize the overall predictive error means (SSE) by adjusting the connection weights. For this network structure the SSE can be written as:
\begin{equation}
SSE = \sum_{i=p+m}^N(y_i-\hat{y}_i)
\end{equation}

\noindent where $\hat{y}_i$ is the output from the network. The number of input nodes corresponds to the number of lagged observations. Having too few or too many input nodes can affect the predictive ability of the neural network \cite{Pao2007}.

\subsubsection{Box-Cox regressor}

The ordinary least squares regression assumes normal distribution of residuals. However, when this is not the case the Box-Cox Regression may be useful \cite{Box1964}. It transforms the dependent variable using the Box-Cox Transformation fnction, and employs maxium likelihood estimatino to determine the optimal level of the power parameter lambda. The Box-Cox Regression requires that no dependent variable has any negative values.

Variable selection and ordinary least qquares output diagogues are identical to that of linear regression. 

The Box-Cox regression will transform the dependent variable as follows:

\begin{equation}
	y^{(\lambda)} = \frac{y^{\lambda}-1}{\lambda}\:if\:\lambda\neq0
\end{equation}
\begin{equation}
	y^{(\lambda)} = Ln(y)\; if\: \lambda=0
\end{equation}

and determine the optimal value of lambda by maximising the following log-likelihood function:

\begin{equation}
	L^{(\lambda)}=-\frac{n}{2}Ln(\hat{\sigma}^2_{(\lambda)}+(\lambda - 1)\sum_{i=1}^nLn(y_i)
\end{equation}

\noindent where $\hat{\sigma}^2_{(\lambda)}$ is the estimate of the least squares variance using the transformed y variable. 

\subsubsection{Passive-Aggressive regressor}

The goal of the Passive-Aggressive (PA) algorithm is to change the model as little as possible to correct for any mistakes and low-confidence predictions it encounters \cite{Gzik2014}. Specficially, with each example PA solves the following optimisation \cite{Ma2009}:

\begin{align}
	\boldsymbol{w}_{t+1}\leftarrow argmin \frac{1}{2}\left|\left|{\boldsymbol{w}_t-\boldsymbol{w}}\right|\right|^2 \\
	s.t. \; \; y_i(\boldsymbol{w}\cdot \boldsymbol{x}_t)\geq1.
\end{align}

\noindent Updates occur when the inner product does not exceed a fixed confidence margin - i.e., $y_i(\boldsymbol{w}\cdot \boldsymbol{x}_t)\geq1$. The closed-form update for all examples is as follows:
\begin{equation}
	\boldsymbol{w}_{t+1}\leftarrow \boldsymbol{w}_{t} + \alpha_t y_t \boldsymbol{x}_t
\end{equation}

\noindent where $\alpha_t=max\left\{\frac{1-y_t(\boldsymbol{w}_t\cdot\boldsymbol{x}_t)}{\left|\left|\boldsymbol{x}_t\right|\right|^2},0\right\}$. Full details of the derivation can be found in \cite{Gzik2014}.

\subsection{Long-term Energy Market Model}


In order to test the impact of the different residual distributions we used the model develop by Kell \textit{et al}., ElecSim \cite{Kell,Kell2020}. ElecSim is an agent-based model which mimic the behaviour of decentralised electricity markets. In this paper we parametrised the model with data of the United Kingdom in 2018. This enabled us to create a digital twin of the UK electricity market and project forward. The data used for this parametrisation included power plants in operation in 2018 and the funds available to the generation companies \cite{dukes_511, companies_house}.

ElecSim is made up of six fundamental components: 1) power plant data; 2) scenario data; 3) the time-steps of the algorithm; 4) the power exchange; 5) the investment algorithm and 6) the generation companies (GenCos) as agents. ElecSim uses a subset of representative days of  electricity demand, solar irradiance and wind speed to approximate a full year. In this context, representative days are a subset of days which when scaled up represent an entire year \cite{Kell2020}. We show how these components interact in Figure \ref{fig:model_details} \cite{Kell}. 



The configuration file details the scenario which can be set by the user. This includes electricity demand, carbon price and fuel prices. The data sources parametrise the digital twin to a particular country, including information such as wind capacity and power plants in operation. Generation Companies own and invest in power plants. These power plants are then matched to electricity demand using a spot market.



\begin{figure}
	\includegraphics[width=0.48\textwidth,natwidth=610,natheight=400]{figures/methods/System_overview_large.png}
	\caption{System overview of ElecSim \cite{Kell}.}
	\label{fig:model_details}
\end{figure}


The market runs a merit-order dispatch model and bids are made by the power plant's short-run marginal cost (SRMC). Investment in power plants are based upon a net present value (NPV) calculation. NPV is able to evaluate and compare investments with cash flows spread over many years. This is shown in Equation \ref{eq:npv_eq}, where $t$ is the year of the cash flow, $i$ is the discount rate, $N$ is the total number of years, or lifetime of power plant, and $R_t$ is the net cash flow of the year $t$:
\begin{equation} \label{eq:npv_eq}
NPV(t, N) = \sum_{t=0}^{N}\frac{R_t}{(1+t)^t}.
\end{equation}

Each of the GenCos estimate the yearly income for each prospective power plant by running a merit-order dispatch electricity market 10 years into the future. However, it is true that the expected cost of electricity 10 years into the future is particularly difficult to predict. We therefore use a reference scenario projected by the UK Government Department for Business and Industrial Strategy (BEIS), and use the predicted costs of electricity calibrated by Kell \textit{et al} \cite{DBEIS2019, Kell2020}. The agents predict the future carbon price by using a linear regression model.


 
\section{Methods}
\label{sec:methods}

%In this section we present the methodology for the approach taken in this paper. The work here was run on a MacBook Pro with a 2.3GHz 8-Core Intel Core i9 processor, with 32GB 2667 MHz DDR4 RAM.

\subsection{Data preparation}

Similarly to the work by Kell \textit{et al.} \cite{Kell2018a}, we selected a number of calendar attributes from the following dataset \cite{gbnationalgridstatus_2019}. This dataset contained data between the years 2011-2018 for the United Kingdom. The calendar attributes used as predictors to the models were hour, month, day of the week, day of the month and year. These attributes allow us to take into acccount the periodicity of the data within each day, month and year.

It is also the case that electricity demand on a public holiday which falls on a weekday falls is dissimilar to load behaviours of ordinary weekdays \cite{Kim2000}. We therefore passed a holiday variable to allow the model to account for this.

In addition to this, as time-series demand data is correlated in the time-domain, we lagged the input demand data. This enabled us to take into account correlations on previous days, weeks and the previous month. Specifically, we used the previous 28 hours before the time step to be predicted for the previous 1st, 2nd, 7th and 30th day. We chose this as we believe that the previous 2 days were the most relevant to the day to be predicted, as well as the weekday of the previous week and the previous month. We could have increased the number of days provided to the algorithm, however due to time and computational constraints we used our previously described intuition for lagged data selection. Future work could be in selecting the optimum number of lagged inputs.

In addition to this, we gave the data information with regards to six different seasons. These seasons were defined by the National Grid Short Term Operating Reserve (STOR) Market Information Report \cite{ESO2019}. Finally, to predict a full 24-hours ahead, we used 24 different models, 1 for each hour of the day. 

The data is standardized and normalized using min-max scaling between -1 and 1 before training and predicting with the model.

\subsection{Algorithm Tuning}

Many machine learning and statistical models have different parameters to train, known as hyperparameters. To find the optimum hyperparameters, cross-validation was used. Cross-validation is the process of training the model on a certain subset of data, and testing it with the remaining subset. As this time-series data was correlated in the time-domain, we took the first 6 years of data (2011-2017) for training and tested on the remaining year of data (2017-2018).

Each machine learning algorithm has a different set of parameters to tune. To tune the parameters in this paper, we used a grid search method. Grid search is effectively a brute force approach that trials each combination of parameters at our choosing.

Tables \ref{table:hyperparameter-tuning-offline} and \ref{table:hyperparameter-tuning-online} display each of the models and respective parameters that were used in the grid search. Table \ref{table:hyperparameter-tuning-offline} shows the offline machine learning methods, whereas Table \ref{table:hyperparameter-tuning-online} displays the online machine learning methods. Each of the parameters within the columns ``Values'' are trialled with every other parameter.

Whilst there is room to increase the total number of parameters, due to the exponential nature of grid-search, we chose a smaller subset of hyperparameters to choose, and a larger number of regressor types.




% Please add the following required packages to your document preamble:
% \usepackage{booktabs}
\begin{table*}[]
\begin{tabular}{@{}lllllll@{}}
\toprule
\textbf{Regressor Type} & \textbf{Parameters} & \textbf{Values}   & \textbf{Parameters} & \textbf{Values} & \textbf{Parameters} & \textbf{Values}       \\ \midrule
Linear                  & N/A                 & N/A               &                     &                 &                     &                       \\
Lasso                   & N/A                 & N/A               &                     &                 &                     &                       \\
Elastic Net             & N/A                 & N/A               &                     &                 &                     &                       \\
Least-Angle             & N/A                 & N/A               &                     &                 &                     &                       \\
Extra Trees             & \# Estimators       & {[}16, 32{]}      &                     &                 &                     &                       \\
Random Forest           & \# Estimators       & {[}16, 32{]}      &                     &                 &                     &                       \\
AdaBoost                & \# Estimators       & {[}16, 32{]}      &                     &                 &                     &                       \\
Gradient Boosting       & \# Estimators       & {[}16, 32{]}      & learning rate       & {[}0.8, 1.0{]}  &                     &                       \\
Support Vector          & Kernel              & {[}linear, rbf{]} & C                   & {[}1, 10{]}     & Gamma               & {[}0.001, 0.0001{]}   \\
Multilayer Perceptron   & Activation function & {[}tanh, relu{]}  & hidden layer sizes  & {[}1, 50{]}     & Alpha               & {[}0.00005, 0.0005{]} \\
K-Neighbours            & \# Neighbours       & {[}5, 20, 50{]}   &                     &                 &                     &                       \\ \bottomrule
\end{tabular}
\caption{Hyperparameters for offline machine learning regression algorithms}
\label{table:hyperparameter-tuning-offline}
\end{table*}



% Please add the following required packages to your document preamble:
% \usepackage{booktabs}
\begin{table*}[]
\begin{tabular}{@{}llp{2.5cm}lllp{1.6cm}@{}}
\toprule
\textbf{Regressor Type} & \textbf{Parameters} & \textbf{Values}                                  & \textbf{Parameters} & \textbf{Values}   & \textbf{Parameters} & \textbf{Values}        \\ \midrule
Linear                  & N/A                 & N/A                                              &                     &                   &                     &                        \\
Box-Cox                 & Power               & {[}0.1, 0.05, 0.01{]}                            &                     &                   &                     &                        \\
Softmax                 & N/A                 & N/A                                              &                     &                   &                     &                        \\
Multilayer Perceptron   & Hidden layer sizes  & {[}(10, 50, 100), (10),  (20), (50), (10, 50){]} & 
                    &                   &                     &                        \\ 
                    Passive Aggressive      & C                   & {[}0.1, 1, 2{]}                                  & Fit intercept?      & {[}True, False{]} & Max iterations      & {[}1, 10, 100, 1000{]} \\
\bottomrule
\end{tabular}
\caption{Hyperparameters for online machine learning regression algorithms}
\label{table:hyperparameter-tuning-online}
\end{table*}



\subsection{Prediction Residuals in ElecSim}

Each of the previously mentioned models trialled will have a certain amount of errors. Prediction residuals is the difference between the estimated and actual values. We collect the prediction residuals to form a distribution for each of the models. We then trial 80 different closed-form distributions to see which of the distributions best fits the residuals from each of the models.

Once each of the prediction residual distributions are fit with a sensible closed-form distribution, we sample from this new distribution and perturb the demand for the electricity market at each time step within ElecSim.

By perturbing the market by the residuals, we are able to observe what the effects are of incorrect predictions of demand in an electricity market using the long-term electricity market model, ElecSim. We are able to understand the differences that prediction residuals have on long-term investment decisions as well as generators utilised.




\section{Results}
\label{sec:results}

In this section we detail the accuracy of the algorithms and statistical models to predict 24 hours ahead for the day ahead market. In addition to this, we display the impact of the errors on electricity generation investment and electricity mix from the years 2018 to 2035 using the agent-based model ElecSim.

\subsection{Offline Machine Learning}

Figure \ref{fig:beis_elecsim_historic_comparison} displays the mean absolute error of each of the offline statistical and machine learning models on a log scale. It can be seen that the different models have varying degrees of success. The least accurate models were linear regression, the multilayer perceptron (MLP) model and the Least Angle Regression (llars). These all have mean absolute errors over $10,000$MWh. This error would be prohibitively high in practice; the max tendered natoinal grid reserve is 6,000MWh, while the average tendered national grid reserve is 2,000MWh \cite{ESO2019}.

A number of models, however, perform well, with a low mean absolute errors. These include the Lasso, gradient Boosting Regressor and K neighbours regressor. The best model, similarly to \cite{Kell2018a}, was the decision tree based model, Extra Trees Regressor, with a mean absolute error of $1604$MWh. This level is well within the average national grid reserve of 2,000MWh.

\begin{figure}
\centering
\includegraphics[width=\columnwidth]{figures/results/offline_model_mae.eps}
\caption{Offline models mean absolute error comparison, with 95\% confidence interval for 5 runs of each model.}
\label{fig:beis_elecsim_historic_comparison}
\end{figure}


%\begin{figure}
%\centering
%\includegraphics[width=\columnwidth]{figures/results/offline_rf_actual_predicted.eps}
%\caption{Best offline machine learning learning algorithm (Extra Trees Regressor) predictions versus actuals for a week in June 2018.}
%\label{fig:best_offline_learning_day_simulation}
%\end{figure}

Figure \ref{fig:best_offline_learning_day_distribution} displays the distribution of the best offline machine result (Extra Trees Regressor), as previously discussed. It can be seen that the max tendered national grid reserve falls well above the 5\% and 95\% percentiles. However, there are occasions, where the errors are greater than the maximum tendered national grid reserve. In addition, the majority of the time, the models predictions fall within the average available tendered national grid reserve.

\begin{figure}
\centering
\includegraphics[width=\columnwidth,natwidth=500,natheight=500]{figures/results/ExtraTreesRegressor_distribution_plot.eps}
\caption{Best offline machine learning learning algorithm (Extra Trees Regressor) distribution.}
\label{fig:best_offline_learning_day_distribution}
\end{figure}


\subsection{Online Machine Learning}

To see if we can improve on the predictions we utilize an online machine learning approach. If we are successful, we should be able to reduce the national grid reserves, reducing cost and emissions.

Figure \ref{fig:online_model_mae_barplot} displays the comparison of mean absolute errors for the different trialled online regressor models. To produce this graph, we showed the various hyperparameter trials. Where the hyperparameters had the same results we removed them. For the multilayer perceptron (MLP), we aggregated all hyperparameters, due to the similar nature of the predictions.

It can be seen that the best performing model was the Box-Cox regressor, with a MAE of 1100. This is an improvement of over 30\% on the best offline model. The other models perform less well, however, it can be seen that the linear regression model improves significantly for the online case, when compared to the offline case. The passive aggressive (PA) model improve significantly with the varying parameters, and the MLP performs poorly in all cases.

\begin{figure}
%\centering
\includegraphics[width=\columnwidth,natwidth=1300,natheight=1300]{figures/results/online_model_mae_barplot.eps}
\caption{Comparison of mean absolute errors (MAE) for different online regressor models (identical MAEs removed).}
\label{fig:online_model_mae_barplot}
\end{figure}

Figure \ref{fig:best_online_learning_day_distribution} displays the best online model. We can see a significant improvement over the best online model distribution, shown in Figure \ref{fig:best_offline_learning_day_distribution}. We remain within the max tendered national grid reserve for the vast majority of the time, and the average available tendered national grid reserve is close to the 5\% and 95\% percentiles.



\begin{figure}
\centering
\includegraphics[width=\columnwidth,natwidth=500,natheight=500]{figures/results/online_learning_dists-power-0.1.eps}
\caption{Best online model (Box-Cox Regressor) distribution.}
\label{fig:best_online_learning_day_distribution}
\end{figure}

Figure \ref{fig:bad_online_learning_day_distribution} displays the residuals for a model with poor predictive ability, the passive aggressive regressor. It displays a large period of time of prediction errors at -20000MWh, and often falls outside of the national grid reserve. These results demonstrate the importance of trying a multitude of different models and parameters to improve prediction accuracy.

\begin{figure}
\centering
\includegraphics[width=\columnwidth,natwidth=500,natheight=500]{figures/results/online_learning_dists-PA-regressor-C-2-fit_intercept-false-max_iter-1000-shuffle-false-tol-0.001.eps}
\caption{Online machine learning learning algorithm distribution. (Passive Aggressive Regressor (C=2, fit intercept = false, maximum iterations = 1000, shuffle = false, tolerance = 0.001).}
\label{fig:bad_online_learning_day_distribution}
\end{figure}




\subsection{Scenario Comparison}

In this section we explore the effect of these residuals on investments made and the electricity generation mix.

\subsubsection{Mean Contributed Energy Generation}


Figure \ref{fig:contributed_PV_mean_output} displays the mean photovoltaic (PV) contributed between 2018 and 2035 vs. men absolute error of the various online regressor models displayed in Figure \ref{fig:online_model_mae_barplot}. A positive correlation can be seen with PV contributed and mean absolute error. This is similar for coal and nuclear output, shown in Figures \ref{fig:contributed_Coal_mean_output} and \ref{fig:contributed_Nuclear_mean_output} respectively. However, as shown by Figure \ref{fig:contributed_Offshore_mean_output.eps}, offshore wind reduces with mean absolute error. Figure \ref{fig:contributed_Recip_gas_mean_output} displays the mean reciprocal gas engine output vs. mean absolute error between the same time period. Output for the reciprocal gas engine also increases with mean absolute error.

The reciprocal gas engine was expected to increase with times of high error. This is because, traditionally, reciprocal gas engines are peaker power plants. Peaker power plants provide power at times of peak demand, which can't be covered by other plants due to them being at their maximum capacity level or out of service.

It is hypothesised that coal and nuclear output increase to cover the predicted increased demands of the service. As these generation types are dispatchable, meaning operators can choose when they generate electricity, they are more likely to be used in times of higher predicted demand.

Photovoltaics may be used more with higher errors due to the times at which the errors were greatest. For example, during the day, where demand is higher, as is solar irradiance.


\begin{figure*}
%\centering
\begin{subfigure}[b]{0.475\textwidth}
\centering
\includegraphics[width=\columnwidth]{figures/results/elecsim_results/contributed_PV_mean_output}
\caption{Photovoltaic output.}
\label{fig:contributed_PV_mean_output}
\end{subfigure}
\hfill
\begin{subfigure}[b]{0.475\textwidth}  
%\centering
\includegraphics[width=\columnwidth]{figures/results/elecsim_results/contributed_Coal_mean_output.eps}
\caption{Coal output.}
\label{fig:contributed_Coal_mean_output}
\end{subfigure}
\vskip\baselineskip
\begin{subfigure}[b]{0.475\textwidth}   
%\centering
\includegraphics[width=\columnwidth]{figures/results/elecsim_results/contributed_Nuclear_mean_output.eps}
\caption{Nuclear output.}
\label{fig:contributed_Nuclear_mean_output}
\end{subfigure}
\quad
\begin{subfigure}[b]{0.475\textwidth}   
%\centering
\includegraphics[width=\columnwidth]{figures/results/elecsim_results/contributed_Offshore_mean_output.eps}
\caption{Offshore output.}
\label{fig:contributed_Offshore_mean_output.eps}
\end{subfigure}
\label{fig:pv_coal_nuclear_offshore_outputs}
\caption{Mean outputs of various technologies vs. mean absolute error from 2018 to 2035 in ElecSim.}
\end{figure*}
    
    
    
%
%
%\begin{figure}
%\centering
%\includegraphics[width=\columnwidth]{figures/results/elecsim_results/contributed_Coal_mean_output.eps}
%\caption{Mean coal output vs. mean absolute error from 2018 to 2035 in ElecSim.}
%\label{fig:contributed_Coal_mean_output}
%\end{figure}
%
%
%\begin{figure}
%\centering
%\includegraphics[width=\columnwidth]{figures/results/elecsim_results/contributed_Nuclear_mean_output.eps}
%\caption{Mean nuclear output vs. mean absolute error from 2018 to 2035 in ElecSim.}
%\label{fig:contributed_Nuclear_mean_output}
%\end{figure}
%
%
%\begin{figure}
%\centering
%\includegraphics[width=\columnwidth]{figures/results/elecsim_results/contributed_Offshore_mean_output.eps}
%\caption{Mean offshore output vs. mean absolute error from 2018 to 2035 in ElecSim.}
%\label{fig:contributed_Offshore_mean_output.eps}
%\end{figure}

%\begin{figure}
%\centering
%\includegraphics[width=\columnwidth]{figures/results/elecsim_results/contributed_Onshore_mean_output.eps}
%\caption{Mean onshore output vs. mean absolute error from 2018 to 2035 in ElecSim.}
%\label{fig:contributed_Onshore_mean_output.eps}
%\end{figure}


%\begin{figure}
%\centering
%\includegraphics[width=\columnwidth]{figures/results/elecsim_results/contributed_PV_mean_output}
%\caption{Mean photovoltaic output vs. mean absolute error from 2018 to 2035 in ElecSim.}
%\label{fig:contributed_PV_mean_output}
%\end{figure}




\begin{figure}
\centering
\includegraphics[width=\columnwidth]{figures/results/elecsim_results/contributed_Recip_gas_mean_output.eps}
\caption{Mean reciprocal gas engine output vs. mean absolute error from 2018 to 2035 in ElecSim.}
\label{fig:contributed_Recip_gas_mean_output}
\end{figure}




\subsubsection{Total Energy Generation}



In this section we detail difference in total technologies invested in over the time period between 2018 to 2035, as predicted by ElecSim.

CCGT, Onshore, and reciprocal gas engines are invested in less over the time period, as shown by Figures \ref{fig:total_CCGT_mean_output}, \ref{fig:total_Offshore_mean_output}, \ref{fig:total_Recip_gas_mean_output.eps} respectively. While coal, offshore, nuclear and photovoltaics all exhibit increasing investments.

It is hypothesised that coal and nuclear increase in investment due to their dispatchable nature. While onshore, non-dispatchable by nature, become a less attractive investment when compared to the other technologies.

CCGT and reciprocal gas engines may have decreased in capacity over this time, due to the increase in coal. This could be because of the consistent large errors in prediction accuracy that meant that reciprocal gas engines were perceived to be less value.


\begin{figure*}[H]
%\centering
\begin{subfigure}[b]{0.475\textwidth}
\centering
\includegraphics[width=\columnwidth]{figures/results/elecsim_results/total_CCGT_mean_output.eps}
\caption{Total CCGT.}
\label{fig:total_CCGT_mean_output}
\end{subfigure}
\hfill
\begin{subfigure}[b]{0.475\textwidth}  
%\centering
\includegraphics[width=\columnwidth]{figures/results/elecsim_results/total_Coal_mean_output.eps}
\caption{Total Coal.}
\label{fig:total_Coal_mean_output}
\end{subfigure}
\vskip\baselineskip
\begin{subfigure}[b]{0.475\textwidth}   
\includegraphics[width=\columnwidth]{figures/results/elecsim_results/total_Onshore_mean_output.eps}
\caption{Total Onshore.}
\label{fig:total_Onshore_mean_output}
\end{subfigure}
\quad
\begin{subfigure}[b]{0.475\textwidth}   
%\centering
\includegraphics[width=\columnwidth]{figures/results/elecsim_results/total_Offshore_mean_output.eps}
\caption{Total Offshore.}
\label{fig:total_Offshore_mean_output}
\end{subfigure}
\caption{Total technologies invested in vs. mean absolute error from 2018 to 2035 in ElecSim.}
\end{figure*}




Figure \ref{fig:Carbon_emitted_mean_output} shows an increase in relative mean carbon emitted. This could be due to the increase output of utility of the reciprocal gas engine, coal, and decrease in offshore output.



\begin{figure*}[H]
%\centering
\begin{subfigure}[b]{0.475\textwidth}
\includegraphics[width=\columnwidth]{figures/results/elecsim_results/total_Nuclear_mean_output.eps}
\caption{Total nuclear.}
\label{fig:total_Onshore_mean_output}
\end{subfigure}
\hfill
\begin{subfigure}[b]{0.475\textwidth}  
%\centering
\centering
\includegraphics[width=\columnwidth]{figures/results/elecsim_results/total_PV_mean_output.eps}
\caption{Total photovoltaics.}
\label{fig:total_PV_mean_output}
\end{subfigure}
\vskip\baselineskip
\begin{subfigure}[b]{0.475\textwidth}   
\includegraphics[width=\columnwidth]{figures/results/elecsim_results/total_Recip_gas_mean_output.eps}
\caption{Total Reciprocal gas engine.}
\label{fig:total_Recip_gas_mean_output.eps}
\end{subfigure}
\quad
\begin{subfigure}[b]{0.475\textwidth}   
\includegraphics[width=\columnwidth]{figures/results/elecsim_results/Carbon_emitted_mean_output.eps}
\caption{Mean carbon emitted.}
\label{fig:Carbon_emitted_mean_output}
\end{subfigure}
\caption{a), b), c) Investments in various technologies vs. mean absolute error from 2018 to 2035 in ElecSim and d) mean carbon emissions between 2018 and 2035.}
\end{figure*}






%\begin{figure}
%\centering
%\includegraphics[width=\columnwidth]{figures/results/elecsim_results/total_CCGT_mean_output.eps}
%\caption{Total CCGT invested in vs. mean absolute error from 2018 to 2035 in ElecSim.}
%\label{fig:total_CCGT_mean_output}
%\end{figure}
%
%
%\begin{figure}
%\centering
%\includegraphics[width=\columnwidth]{figures/results/elecsim_results/total_Coal_mean_output.eps}
%\caption{Total Coal invested in vs. mean absolute error from 2018 to 2035 in ElecSim.}
%\label{fig:total_Coal_mean_output}
%\end{figure}
%
%
%
%\begin{figure}
%\centering
%\includegraphics[width=\columnwidth]{figures/results/elecsim_results/total_Offshore_mean_output.eps}
%\caption{Total Offshore invested in vs. mean absolute error from 2018 to 2035 in ElecSim.}
%\label{fig:total_Offshore_mean_output}
%\end{figure}





%\begin{figure}
%\centering
%\includegraphics[width=\columnwidth]{figures/results/elecsim_results/total_Onshore_mean_output.eps}
%\caption{Total Onshore invested in vs. mean absolute error from 2018 to 2035 in ElecSim.}
%\label{fig:total_Onshore_mean_output}
%\end{figure}


%\begin{figure}
%\centering
%\includegraphics[width=\columnwidth]{figures/results/elecsim_results/total_PV_mean_output.eps}
%\caption{Total photovoltaics invested in vs. mean absolute error from 2018 to 2035 in ElecSim.}
%\label{fig:total_PV_mean_output}
%\end{figure}

%\begin{figure}
%\centering
%\includegraphics[width=\columnwidth]{figures/results/elecsim_results/total_Recip_gas_mean_output.eps}
%\caption{Total Reciprocal gas engine invested in vs. mean absolute error from 2018 to 2035 in ElecSim.}
%\label{fig:total_Recip_gas_mean_output.eps}
%\end{figure}
















%- Include residuals and MAE,MAPE,MASE etc\\


\section{Discussion}
\label{sec:discussion}

%- Discuss the impact of this on the electricity market and global economy. Make suggestions.

From our results it can be seen that different algorithms yield differing prediction accuracies. Online models results in a decrease in 30\% of prediction error on the best offline models. We therefore recommend the use of online machine learning for predicting electricity demand in a day ahead market.

Additionally, the impact on the wider electricity market has been shown to be significant. Principally, the investment behaviours of generation companies change as well as the dispatched electricity mix. The relative mean carbon emitted over this time period increases, due to an increase in the utilisation of coal and reciprocal gas engines, at the expense of offshore wind.


\section{Conclusion}
\label{sec:conclusion}

In this paper, we trialled multiple different machine learning and statistical models to predict electricity demand in the UK for the day-ahead market. Specifically, we used both online and offline algorithms to predict electricity demand 24 hours ahead. 

We then measured the errors and compared these to each model as well as the national grid reserve. We found that through the use of an online learning approach we were able to significantly reduce error by 30\% on the best offline algorithm. We were also able to reduce our errors to significantly below the national grid's mean and maximum tendered reserve.

In addition to this, we took these errors, or residuals, and perturbed the electricity market of the agent-based model ElecSim. This enabled us to see the impact of different error distributions on the long-term electricity market, both in terms of investment and in terms of electricity mix.

In future work we would like to trial a different selection of algorithms and statistical models and trial different inputs to the models. For instance, by giving 2 months worth of historical data.

\section{Funding Sources}

This work was supported by the Engineering and Physical Sciences Research Council, Centre for Doctoral Training in Cloud Computing for Big Data [grant number EP/L015358/1].





%% The Appendices part is started with the command \appendix;
%% appendix sections are then done as normal sections
%% \appendix

%% \section{}
%% \label{}

%% If you have bibdatabase file and want bibtex to generate the
%% bibitems, please use
%%
  \bibliographystyle{elsarticle-num} 
  \section*{References}
  \bibliography{library,bib_custom,custombibtex}

%% else use the following coding to input the bibitems directly in the
%% TeX file.

%\begin{thebibliography}{00}
%
%%% \bibitem[Author(year)]{label}
%%% Text of bibliographic item
%
%\bibitem[ ()]{}
%
%\end{thebibliography}
\end{document}

\endinput
%%
%% End of file `elsarticle-template-harv.tex'.
	